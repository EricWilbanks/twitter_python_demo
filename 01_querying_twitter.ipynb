{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "import csv\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "\n",
    "# pw.py contains your secret api keys\n",
    "# these values should NEVER be shared with others\n",
    "# this information could be use to hijack your account and post anything\n",
    "# we're importing the values we need from the pw.py file; they are not encoded or protected\n",
    "# if you compile this notebook and then share it with someone, they could access those values\n",
    "# make SURE that you \"restart the notebook kernel and clear all outputs\" or redefine the values before sharing the notebook\n",
    "\n",
    "from pw import tw_consumer_key, tw_consumer_secret, tw_access_token, tw_access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up api connection\n",
    "auth = tw.OAuthHandler(tw_consumer_key, tw_consumer_secret)\n",
    "auth.set_access_token(tw_access_token, tw_access_token_secret)\n",
    "api = tw.API(auth, wait_on_rate_limit = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Rate Limits\n",
    "\n",
    "To protect their servers, Twitter limits the number and size of requests each account can make through the API.\n",
    "\n",
    "The full details are here: https://developer.twitter.com/en/docs/twitter-api/v1/rate-limits\n",
    "\n",
    "For our purposes today, I've already made some searches and saved them to csv files (detailed below). If you're working through this demo asynchronously, you'll have to collect your own tweets to work through.\n",
    "\n",
    "For more in-depth projects, you'll likely want to utilize the Twitter streaming API which allows continuous queries over longer periods of time. (see an example here: http://docs.tweepy.org/en/latest/streaming_how_to.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweepy Basic Search\n",
    "\n",
    "The example below shows a typical search using Tweepy's Cursor method. In this example, we're searching for the first *1000 tweets* (as defined in the .items() method) that match our search criteria.\n",
    "\n",
    "They are:\n",
    "1. Contains the word 'snow' and is not a retweet (the query, \"-q\")\n",
    "1. Are tagged as English\n",
    "1. Occurred since Oct 20, 2020\n",
    "\n",
    "The Tweepy search parameters and Twitter query are quite powerful and versatile. Here are some links to useful resources!\n",
    "\n",
    "- Twitter's search queries syntax [https://developer.twitter.com/en/docs/labs/recent-search/guides/search-queries]\n",
    "    - this goes into the -q argument of a tweepy call\n",
    "- Tweepy's various search parameters [http://docs.tweepy.org/en/latest/api.html#search-methods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for first 1000 tweets containing the term 'snow', ignoring retweets, in English, since Oct 20 2020\n",
    "snow_tweets = tw.Cursor(api.search,\n",
    "                         q = 'snow -filter:retweets',\n",
    "                         lang = 'en',\n",
    "                         since = '2020-10-20').items(1000)\n",
    "\n",
    "# convert to list so we can manipulate it more easily\n",
    "snow_list = list(snow_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the internal structure of a tweet entry in the list\n",
    "tweet = snow_list[1]\n",
    "print([tweet.text, tweet.user.id_str, tweet.user.name, tweet.id_str, tweet.created_at])\n",
    "\n",
    "# entire structure\n",
    "vars(snow_list[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Method 1: Saving Specific Information\n",
    "\n",
    "The tweets returned from the API contain a wealth of information, and may be too detailed depending on your specific question.\n",
    "\n",
    "If you know which fields you'd like to retain, you can save just those particular attributes as columns in a csv file as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## export certain information in tweets to csv file \n",
    "\n",
    "with open('./saved_searches/snow_tweets.csv', 'a') as output_file:\n",
    "    csvWriter = csv.writer(output_file)\n",
    "    csvWriter.writerow(['text','user.id_str','user','id_str','created_at'])\n",
    "    # iterate over list of tweets and save text, user id, user name, tweet id, and creation time\n",
    "    for tweet in snow_list:\n",
    "        csvWriter.writerow([tweet.text, tweet.user.id_str, tweet.user.name, tweet.id_str, tweet.created_at])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Saving all .json information in a csv file\n",
    "\n",
    "If you'd like to save all of the various attributes for each tweet, you can utilize the `_json` attribute of each tweepy object. .json is a widely used data structure for storing text. All of the attributes of a tweet (e.g., its content, author, time created, etc.) are stored in .json format in the tweet's `_json` attribute.\n",
    "\n",
    "We'll convert these json entries into pandas dataframe representation, and then save the entire dataframe to a csv.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert entire tweet entry into pandas data frame (and save that)\n",
    "\n",
    "# create empty list\n",
    "rows = []\n",
    "\n",
    "# loop over all the tweepy objects in our `snow_list`\n",
    "for tweet in snow_list:\n",
    "    # convert the json data in the tweepy object into a pandas dataframe row, and add to our list\n",
    "    rows.append(pd.json_normalize(tweet._json))\n",
    "\n",
    "# concatenate all of the individual entries\n",
    "snow_tweets_df = pd.concat(rows, ignore_index = True)\n",
    "\n",
    "# save the dataframe to a csv file for later\n",
    "snow_tweets_df.to_csv('./saved_searches/snow_tweets_extensive.csv', index = False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Searches\n",
    "\n",
    "The following contain additional searches that we'll be using for different purposes in the workshop.\n",
    "\n",
    "Try interpreting the types of types we're filtering for based on the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pillow_tweets = tw.Cursor(api.search,\n",
    "                         q = 'pillow -filter:retweets',\n",
    "                         lang = 'en').items(10000)\n",
    "\n",
    "# convert to list so we can manipulate it more easily\n",
    "pillow_list = list(pillow_tweets)\n",
    "\n",
    "# export certain information in tweets to csv file \n",
    "\n",
    "with open('./saved_searches/pillow_tweets.csv', 'a') as output_file:\n",
    "    csvWriter = csv.writer(output_file)\n",
    "    csvWriter.writerow(['text','user.id_str','user','id_str','created_at'])\n",
    "    # iterate over list of tweets and save text, user id, user name, tweet id, and creation time\n",
    "    for tweet in pillow_list:\n",
    "        csvWriter.writerow([tweet.text, tweet.user.id_str, tweet.user.name, tweet.id_str, tweet.created_at])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beach_tweets = tw.Cursor(api.search,\n",
    "                        q = 'beach -filter:retweets',\n",
    "                        lang = 'en',\n",
    "                        geocode = '39.833,-98.583,1660mi').items(5000)\n",
    "\n",
    "beach_list_full = list(beach_tweets)\n",
    "\n",
    "# filter out only those tweets that have specific geocodes attached\n",
    "beach_list = [t for t in beach_list_full if t.geo is not None]\n",
    "\n",
    "## convert entire tweet entry into pandas data frame (and save that)\n",
    "\n",
    "# create empty list\n",
    "rows = []\n",
    "\n",
    "# loop over all the tweepy objects in our `beach_list`\n",
    "for tweet in beach_list:\n",
    "    # convert the json data in the tweepy object into a pandas dataframe row, and add to our list\n",
    "    rows.append(pd.json_normalize(tweet._json))\n",
    "\n",
    "# concatenate all of the individual entries\n",
    "beach_tweets_df = pd.concat(rows, ignore_index = True)\n",
    "\n",
    "# save the dataframe to a csv file for later\n",
    "beach_tweets_df.to_csv('./saved_searches/beach_tweets_extensive.csv', index = False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_mentions = tw.Cursor(api.search,\n",
    "                          q = '\"Donald Trump\" -filter:retweets',\n",
    "                          lang = 'en',\n",
    "                          result_type = 'recent').items(500)\n",
    "trump_list = list(trump_mentions)\n",
    "\n",
    "biden_mentions = tw.Cursor(api.search,\n",
    "                          q = '\"Joe Biden\" -filter:retweets',\n",
    "                          lang = 'en',\n",
    "                          result_type = 'recent').items(500)\n",
    "biden_list = list(biden_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list\n",
    "rows = []\n",
    "\n",
    "# loop over all the tweepy objects in our `trump_list`\n",
    "for tweet in trump_list:\n",
    "    # convert the json data in the tweepy object into a pandas dataframe row, and add to our list\n",
    "    rows.append(pd.json_normalize(tweet._json))\n",
    "\n",
    "# concatenate all of the individual entries\n",
    "trump_tweets_df = pd.concat(rows, ignore_index = True)\n",
    "\n",
    "# save the dataframe to a csv file for later\n",
    "trump_tweets_df.to_csv('./saved_searches/trump_tweets_extensive.csv', index = False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty list\n",
    "rows = []\n",
    "\n",
    "# loop over all the tweepy objects in our `biden_list`\n",
    "for tweet in biden_list:\n",
    "    # convert the json data in the tweepy object into a pandas dataframe row, and add to our list\n",
    "    rows.append(pd.json_normalize(tweet._json))\n",
    "\n",
    "# concatenate all of the individual entries\n",
    "biden_tweets_df = pd.concat(rows, ignore_index = True)\n",
    "\n",
    "# save the dataframe to a csv file for later\n",
    "biden_tweets_df.to_csv('./saved_searches/biden_tweets_extensive.csv', index = False, quoting=csv.QUOTE_ALL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
