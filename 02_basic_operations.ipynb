{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Operations and Manipulating Tweet Data\n",
    "\n",
    "We'll be using `pandas` to manipulate our saved data as dataframes. If you're familiar with R dataframes, the structure is similar! \n",
    "\n",
    "These dataframes allow us to conduct operations and manipulate rows and columns in an efficient manner, without having to do for loops or other methods.\n",
    "\n",
    "If you're looking for more resources on learning Pandas basics, check out the resources list in `README.md`\n",
    "\n",
    "## Visualizing Tweet Frequency Across Time\n",
    "\n",
    "As linguists we might be interested in studying the temporal distribution of different lexical items. For example, you might study the rise and fall of 'slang' terms over months and years. You could also study spelling variants that possibly reflect dialect differences and how they change in representation in twitter. You might also be interested in how certain lexical items are distributed w.r.t. to certain important events or times of day.\n",
    "\n",
    "Let's visualize the distribution of our 10,000 tweets containing \"pillow\" in `saved_searches/pillow_tweets.csv` in order to test the hypothesis: \n",
    "\n",
    "- **H1: Tweets containing the word \"pillow\" will be more frequent leading up to people's bedtimes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import nltk\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "# download nltk resources if you haven't before (uncomment to do so)\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "\n",
    "# set matplot preference\n",
    "%matplotlib inline\n",
    "\n",
    "# read in data\n",
    "pillow = pd.read_csv('saved_searches/pillow_tweets.csv')\n",
    "\n",
    "# create new column that only includes time information\n",
    "pillow['created_at'] = pd.to_datetime(pillow['created_at'], format = '%Y-%m-%d %H:%M:%S')\n",
    "pillow['time'] = [datetime.datetime.time(i) for i in pillow['created_at']]\n",
    "\n",
    "# view first and last 5 rows\n",
    "print(pillow.head())\n",
    "print(pillow.tail())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API tweets are in UTC time, tweets were recorded during Daylight's Savings Time, so EDT is -4 hours from UTC.\n",
    "# this is implemented by subtracting a datetime.timedelta() of 4 hours\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.hist(pillow['created_at'] - datetime.timedelta(hours = 4), bins=16, histtype='step')\n",
    "ax.xaxis.set_major_formatter(formatter = mdates.DateFormatter('%H:%M'))\n",
    "plt.xticks(rotation = 45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Graph\n",
    "\n",
    "This graph is suggestive, but there are a few issues here that don't allow us to directly address our hypothesis.\n",
    "\n",
    "**Can you think of some? Try brainstorming for a second before looking at some problems below...**\n",
    "\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "- These are just counts! They don't tell us if the relative frequency of \"pillow\" changes following our hypothesis.\n",
    "- The search didn't restrict *where* tweets came from, it's basically assured that not all of these tweets are from EDT users.\n",
    "- \"Pillow\" search can include wider set of items than \"sleeping pillow\", these different senses probably have different distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing\n",
    "\n",
    "Some of the questions we might ask about a tweet's text might be easier if we could remove punctuation, emoji, or function words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Demonstration of method to tokenize and clean sentences\n",
    "\n",
    "# word_tokenize() breaks up a large string into a list containing individual words it has identified\n",
    "s = 'This is an example sentence, with a TON..... of punctuation!!!!And :) a smiley'\n",
    "s_tokenized = nltk.tokenize.word_tokenize(s)\n",
    "print(s_tokenized)\n",
    "\n",
    "# let's convert all of the words to lower case\n",
    "s_lower = [w.lower() for w in s_tokenized]\n",
    "print(s_lower)\n",
    "\n",
    "# now let's remove non-words\n",
    "s_words = [s for s in s_lower if s.isalpha()]\n",
    "print(s_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords\n",
    "\n",
    "A common method to improve analyses of content words in NLP is to remove **stopwords**. These roughly map onto pronouns and function words, and are often ignored for the purpose of entity tagging, sentiment analysis, and other NLP methods.\n",
    "\n",
    "If you're interested in syntactic phenomena or the spelling/frequency of function words, you'll obviously want to keep them. But otherwise, you might consider removing stopwords like we do below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_words)\n",
    "\n",
    "# define the stopwords list\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# only include words that aren't in the stopwords list\n",
    "s_final = [w for w in s_words if not w in stopwords]\n",
    "print(s_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Cleaning Function \n",
    "\n",
    "We want to apply this routine (tokenize --> lower case --> remove non-alpha --> remove stopwords) to all of the text of the tweets we have. Let's define a function to do this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(s, stopwords):\n",
    "    \"\"\"\n",
    "    Take as input a sentence as a str and a list of stopwords\n",
    "    Then output a tokenized, lower case list of all words which are not stopwords.\n",
    "    You could make this function more efficient by vectorizing it and then applying it to a pandas column simultaneously.\n",
    "    \"\"\"\n",
    "    s_tokenized = nltk.tokenize.word_tokenize(s)\n",
    "    s_lower = [w.lower() for w in s_tokenized]\n",
    "    s_words = [w for w in s_lower if w.isalpha()]\n",
    "    s_final = [w for w in s_words if not w in stopwords]\n",
    "    return s_final\n",
    "\n",
    "print(clean_sentence('this!!!is a test OF THE FUNCTION,!;', stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pillow['text'][0])\n",
    "print(clean_sentence(pillow['text'][0], stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this process isn't perfect for tweets. \n",
    "\n",
    "It splits hyperlinks and includes \"https\" in our words list, and also keeps \"amp\" from the symbol \"&amp;\".\n",
    "\n",
    "For our rough purposes though, this will be sufficient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using .apply() and lambda allows us to apply our new function to each row of the dataframe\n",
    "# and save the output as a new column\n",
    "pillow['cleaned_text'] = pillow.apply(lambda row: clean_sentence(row['text'], stopwords), axis = 1)\n",
    "pillow.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Grams\n",
    "\n",
    "A useful representation of ordered sequence data (like sentences/tweets) are N-Grams: \n",
    "\n",
    "In our case, let's consider the bigrams (sequences of two elements) in the following sentence:\n",
    "\n",
    "**\"I fed the dog before I hugged the dog.\"**\n",
    "\n",
    "- I fed\n",
    "- fed the\n",
    "- the dog\n",
    "- dog before\n",
    "- before I\n",
    "- I hugged\n",
    "- hugged the\n",
    "- the dog\n",
    "\n",
    "Notice that some bigrams in this sentence are more common than others (\"the dog\" occurs twice), while some bigrams are unattested (\\*\"dog the\")!\n",
    "\n",
    "<br />\n",
    "\n",
    "Let's consider if the bigrams including \"pillow\" give us any insight into the types of conversations people are having."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all rows of cleaned_text column into a single list\n",
    "all_cleaned_text = pillow['cleaned_text'].explode().dropna().to_list()\n",
    "\n",
    "# construct the nltk bigram finder\n",
    "bigram_measures = BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(all_cleaned_text)\n",
    "\n",
    "# apply filter and only look at bigrams that contain \"pillow\"\n",
    "pillow_filter = lambda *w: 'pillow' not in w\n",
    "finder.apply_ngram_filter(pillow_filter)\n",
    "\n",
    "# print out the 10 most likely bigrams (measured by chi_sq)\n",
    "for l in finder.nbest(bigram_measures.chi_sq, 10):\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to just identifying high-frequency collocations (bigrams), n-gram models are the basis of many computational language models.\n",
    "\n",
    "We won't explore n-grams any further, but check out the resources on NLTK and NLP/N-Grams in the `README.md`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
